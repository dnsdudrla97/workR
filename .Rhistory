#lm(formula = dist ~ speed, data = cars)
#
#Coefficients:
#  (Intercept)        speed
#      -17.579        3.932
#      b값            w값
# y = 3.932*x -17.579
abline(model)
cars
source('D:/workR/1216.R', encoding = 'UTF-8', echo=TRUE)
residuals(model) # 잔차 : 회귀식으로 추정된 값과의 차이
# 잔차 제곱합을 평균제곱오차(MES-mean squared error)로 변환
deviance(model)/length(cars$speed)
b <- coef(model)[1]
w <- coef(model)[2]
b; w
sqrt(227.0704)
coef(model) # 매개변수(계수) = w, b값 출력
plot(dist~speed, data=cars)
plot(cars)
# 회귀모델 구하기,
# 종속 (반응)변수 독립(설명)변수 순서로 지정
model <- lm(dist~speed, cars)
model
#Call:
#lm(formula = dist ~ speed, data = cars)
#
#Coefficients:
#  (Intercept)        speed
#      -17.579        3.932
#      b값            w값
# y = 3.932*x -17.579
abline(model)
coef(model) # 매개변수(계수) = w, b값 출력
fitted(model) # 훈련 data에 있는 샘플에 대한 예측값
residuals(model) # 잔차 : 회귀식으로 추정된 값과의 차이
# 잔차 제곱합을 평균제곱오차(MES-mean squared error)로 변환
deviance(model)/length(cars$speed)
b <- coef(model)[1]
w <- coef(model)[2]
speed <- 21.5
dist <- w*speed + b
dist
df <- data.frame(speed=c(21.5,25.0,25.5,26.0,26.5,27.0,27.5,28.0))
predict(model,df)
plot(df$speed,predict(model,df),col='red',cex=2,pch=20)
abline(model)
speed <- cars[ ,1]
pred <- w*speed + b
pred
compare <- data.frane(pred, cars[ ,2], pred~cars[ ,2])
compare <- data.frame(pred, cars[ ,2], pred~cars[ ,2])
compare
compare <- data.frame(pred, cars[ ,2], pred-cars[ ,2])
compare
colnames(compare) <- c("예상","실제","오차")
head(fitted(model),3) # 예측측
head(residuals(model), 3)
head(compare, 3)
summary(model)
str(cars)
head(cars)
car_model <- lm(dist~speed,data=caars)
car_model <- lm(dist~speed,data=cars)
coef(car_model)
plot(cars)
abline(car_model,col='red')
summary(car?_model)
summary(car_model)
str(cars)
head(cars)
car_model <- lm(dist~speed,data=cars)
coef(car_model)
plot(car_model)
summary(car_model)
str(women)
head(women)
women_model <- lm(weights~height,data=women)
coef(women_model)
women_model <- lm(weights~height,data=women)
women_model <- lm(weight~height,data=women)
coef(women_model)
plot(women_model)
str(cars)
head(cars)
car_model <- lm(dist~speed,data=cars)
coef(car_model)
plot(car_model)
abline(car_model,col='red')
summary(car_model)
str(women)
head(women)
women_model <- lm(weight~height,data=women)
coef(women_model)
plot(women_model)
abline(women_model,col='red')
summary(women_model)
str(cars)
head(cars)
car_model <- lm(dist~speed,data=cars)
coef(car_model)
plot(car_model)
str(cars)
head(cars)
car_model <- lm(dist~speed,data=cars)
coef(car_model)
plot(car_model)
abline(car_model,col='red')
summary(car_model)
plot(car_model)
abline(car_model,col='red')
plot(car_model)
str(women)
head(women)
women_model <- lm(weight~height,data=women)
coef(women_model)
plot(women_model)
abline(women_model,col='red')
summary(women_model)
summary(car_model)
summary(women_model)
st <- data.frame(state.x77)
lm(st)
lm(Illiteracy~Murder,data=st)
lm(Murder~Illiteracy,data=st)
st_model <- lm(Murder~Illiteracy,data=st)
st_model
b <- coef(st_model)[1]
w <- coef(st_model)[2]
Murder <- w*Illiteracy + b
Illiteracy <- c(0.5, 1.0, 1.5)
Murder <- w*Illiteracy + b
Murder
df <- data.frame(Illiteracy = c(0.5, 1.0, 1.5))
predict(st_model,df)
plot(df$Illiteracy,predict(st_model,df),col='red',cex=2,pch=20)
abline(st_model)
st <- data.frame(state.x77)
st_model <- lm(Murder~Illiteracy,data=st)
st_model
b <- coef(st_model)[1]
w <- coef(st_model)[2]
st <- data.frame(state.x77)
st_model <- lm(Murder~Illiteracy,data=st)
st_model
df <- data.frame(Illiteracy = c(0.5, 1.0, 1.5))
predict(st_model,df)
plot(df$Illiteracy,predict(st_model,df),col='red',cex=2,pch=20)
abline(st_model)
trees_model <- lm(Volume~Girth,data=trees)
trees_model
df <- data.frame(Girth = c(83.5,9.0,9.5))
predict(trees_model,df)
plot(df$Girth,predict(trees_model,df),col='red',cex=2,pch=20)
df <- data.frame(Girth = c(8.5,9.0,9.5))
predict(trees_model,df)
plot(df$Girth,predict(trees_model,df),col='red',cex=2,pch=20)
abline(trees_model)
pr_model <- lm(pressure~temperature,data=pressure)
pr_model
df <- data.frame(temperature=c(65,95,155))
predict(pr_model,df)
plot(df$temperature,predict(pr_model,df),col='red',cex=2,pch=20)
abline(pr_model)
#
# 다중선형 회귀분석( multiple linear regression analyze )
#
# 다중선형 회귀모델 : 여러 개의 독립변수를 다루는 회귀모델
#
# 회귀식
# y= B0 + B1X1 + B2X2 + B3X3 + ... + BnXn
#
# 독립변수가 n개인 다중선형 회귀에서 주어진 자료를 이용해
# B0, B1, B2, B3, ..., Bn의 값을 알아내는 회귀모델
library( tidyverse )
library( car )
str( Prestige )
head( Prestige )
new.data <- Prestige[ , c(1:4)]
head(new.data)
plot(newdata, pch=16, col="blue", main = "Metrix Scarrerplot")
plot(new.data, pch=16, col="blue", main = "Metrix Scarrerplot")
newdata <- Prestige[ , c(1:4)]
head(newdata)
plot(newdata, pch=16, col="blue", main = "Metrix Scarrerplot")
str( Prestige )
head( Prestige )
newdata <- Prestige[ , c(1:4)]
head(newdata)
plot(newdata, pch=16, col="blue", main = "Metrix Scarrerplot")
model <- lm( income~education + prestige + women, data=newdata)
model
coef(model)
# 회귀식
# income = (-253.8497) +
#          (177.1990*newdata$education)
#          (141.4354*newdata$prestige)
#          (-50.8957*newdata$women)
fitted(model)
residuals(model)
deviance(model)
deviance(model)/length(newdata$education) # 평균 제곱 오차차
summary(model)
car_model <- lm(dist~speed,data=cars)
coef(car_model)
plot(car_model)
coef(car_model)
plot(car_model)
coef(car_model)
plot(car_model)
plot(women_model)
abline(women_model,col='red')
str(women)
head(women)
women_model <- lm(weight~height,data=women)
coef(women_model)
plot(women_model)
newdata2 <- Prestige[ , c(1:5)]
model2 <- lm(income~.,data=newdata2)
summary(model2)
library(MASS)
model3 <- stepAIC <- stepAIC(model2)
summary(model3)
#
# Logistic Regression(로지스틱 회귀분석)
#  회귀 모델에서 종속변수의 값의 형태가 범주형인 경우 예측모델
#
#  주어진 data로부터 어떤 범주를 예측하는 분야를 회귀와 구분하여 분류(classification)이라고 한다.
#
#  로지스틱 회귀도 기본적으로 회귀 기법이기 때문에 종속변수는 숫자로 표현되어야 한다.
#  예) YES와 NO눈 0과 1로 setosa, versicolor, virginica는 1, 2, 3과 같이 숫자로 바꾼 후에
#     로지스틱 회귀 적용
#
iris.new-iris
#
# Logistic Regression(로지스틱 회귀분석)
#  회귀 모델에서 종속변수의 값의 형태가 범주형인 경우 예측모델
#
#  주어진 data로부터 어떤 범주를 예측하는 분야를 회귀와 구분하여 분류(classification)이라고 한다.
#
#  로지스틱 회귀도 기본적으로 회귀 기법이기 때문에 종속변수는 숫자로 표현되어야 한다.
#  예) YES와 NO눈 0과 1로 setosa, versicolor, virginica는 1, 2, 3과 같이 숫자로 바꾼 후에
#     로지스틱 회귀 적용
#
iris.new <- iris
iris.new$Species <- as.integer(iris.new$Species
)
head(iris.new)
iris_model <- glm(Species~., data=iris.new)
iris_model
coef(iris_model)
summary(iris_model)
unknown <- data.frame(rbind(c(5.1,3.5,1.4,0.2)))
names(unknown) <- names(iris)[1:4]
unknown
pred <- predict(iris_model, unknown)
pred
round(pred, 0)
levels(iris$Species)[pred]
round(pred, 0)
levels(iris$Species)[pred]
pred <- predict(iris_model, unknown)
pred
round(pred, 0)
levels(iris$Species)[pred]
pred <- round(pred, 0)
levels(iris$Species)[pred]
test <- iris[ ,1:4]
pred <- predict(iris_model,test)
pred <- round(pred, 0)
answer <- as.integer(iris$Species)
pred == answer
acc <- mean(pre == answer)
acc
acc <- mean(pred == answer)
acc
str(trees)
head(trees)
treedata <- tree[ ,c(1,2)]
treedata <- trees[ ,c(1,2)]
head(treedata)
str( Prestige )
head( Prestige )
newdata <- Prestige[ , c(1:4)]
head(newdata)
str(trees)
model <- lm(Volume~.,data = trees)
model
ceof(model)
# (1) 나무 둘레(Girth)와 나무의 키(Height)로 나무의 볼륨을 예측하는
# 다중선형 회귀모델을 만드시오.
library( tidyverse )
ceof(model)
library( car )
ceof(model)
pred <- predict(model)
pred
coef(model)
str(trees)
head(trees)
model <- lm(Volume~.,data = trees)
model
coef(model)
# 예측값
pred
# 실제값
fitted(model)
# 잔차
residuals(model)
# 실제값
model
# 실제값
trees$Volume
# 예측값
pred
# 잔차
residuals(model)
str(mtcars)
head(mtcars)
model2 <- lm(mpg~.,data = mtcars)
model2
model2[1]
model2[1,]
model2[1]
coef(model)[1]
coef(model2)[1]
h0 <- coef(model2)[1]
h5 <- coef(model2)[6]
h0 <- coef(model2)[1]
h1 <- coef(model2)[2]
h2 <- coef(model2)[3]
h3 <- coef(model2)[4]
h4 <- coef(model2)[5]
h5 <- coef(model2)[6]
h6 <- coef(model2)[7]
h7 <- coef(model2)[8]
h8 <- coef(model2)[9]
h9 <- coef(model2)[10]
h10 <- coef(model2)[11]
model2
mpg = h0 + h1*cyl + h2*disp + h3*hp + h4*drat + h5*wt +
h6*qsec + h7*vs + h8*am + h9+gear + h10*carb
mpg = h0 + h1*cyl + h2*disp + h3*hp + h4*drat + h5*wt +
h6*qsec + h7*vs + h8*am + h9+gear + h10*carb
mpg = h0 + h1*mtcars$cyl + h2*mtcars$disp + h3*mtcars$hp +
h4*mtcars$drat + h5*mtcars$wt + h6*mtcars$qsec +
h7*mtcars$vs + h8*mtcars$am + h9*mtcars$gear + h10*mtcars$carb
summary(model2)
library(MASS)
model2_1 <-  stepAIC(model2)
summary(model2_1)
mpg = h0 + h5*mtcars$wt + h6*mtcars$qsec + h8*mtcars$am
summary(model2)
summary(model2_1)
mydata <- read.csv( "https://stats.idre.ucla.edu/stat/data/binary.csv" )
str(mydata)
head(mydata)
new_data <- mydata
new_data$admit <- as.integer(mydata$admit)
head(my_data)
head(new_data)
new_model <- glm(admit~.,data = new_data)
new_model
coef(new_model)
summary(new_model)
head(mydata)
known <- mydata[ ,-1]
pred <- predict(new_model,known)
pred
pred <- round(pred, 0)
levels(mydata$admit)[pred]
pred <- round(pred, 0)
levels(mydata$admit)[pred]
levels(new_data$admit)[pred]
pred <- round(pred, 0)
str(mydata)
head(mydata)
new_data <- mydata
new_data$admit <- as.integer(mydata$admit)
head(new_data)
new_model <- glm(admit~.,data = new_data)
new_model
coef(new_model)
known <- mydata[ ,-1]
pred <- predict(new_model,known)
pred
pred <- round(pred, 0)
levels(new_data$admit)[pred]
str(mydata)
head(mydata)
new_data <- mydata
new_data$admit <- as.integer(mydata$admit)
head(new_data)
new_model <- glm(admit~.,data = new_data)
new_model
coef(new_model)
known <- mydata[ ,-1]
pred <- predict(new_model,known)
pred
pred <- round(pred, 0)
levels(mydata$admit)[pred]
pred <- round(pred)
levels(mydata$admit)[pred]
str(mydata)
head(mydata)
new_data <- mydata
head(new_data)
new_model <- glm(admit~.,data = new_data)
new_model
coef(new_model)
known <- mydata[ ,-1]
pred <- predict(new_model,known)
pred
pred <- round(pred)
pred
levels(mydata$admit)[pred]
pred
pred <- predict(new_model,known)
pred
pred <- round(pred,0)
pred
mydata$admit
real <- mydata$admit
real # 실제값
str(iris)
str(mydata)
test <- mydata[ ,2:4]
pred <- predict(new_model,test)
pred <- round(pred,0)
answer <- mydata$admit
pred == answer
acc <- mean(pred == answer)
acc
str(pred)
# 2019-12-18
#
# 군집화(clustering), qnsfb(classification)
#
# 군집화(clustering)
# 주어진 대상 데이터들을 유사성이 높은 것끼리 묶어주는 기술
# (군집, 범주, 그룹)
#
# k-means(평균) 군집화 알고리즘
mydata <- iris[ ,1:4]
fit <- kmeans(x=mydata, centers = 3)
fit
fit$cluster # 군집번호
fit$centers # 군집된 점의 좌표
library(cluster)
clusplot(mydata,        # 군집대상
fit$cluster,   # 군집번호
color = TRUE,  # 원의 색
shade = TRUE,  # 원의 빗급표시 유무
labels = 2,    # 관측값 출력 형태
lines = 1)     # 중심선 연결 표시
subset(mydata, fit$cluster == 2)
std <- function(x) {
return((x - min(A)) / (max(A) - min(A)))
}
mydata <- apply(iris[ ,1:4], 2, std)
return((x - min(x)) / (max(x) - min(x)))
std <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
mydata <- apply(iris[ ,1:4], 2, std)
fit <- kmeans(x = mydata, centers = 3)
fit
#
# KNN(K-Nearest Neighbor, K-최근접 이웃) 분류 알고리즘
#
library(class)
# 훈련용/테스트용 데이터 준비
tr.idx <- c(1:25,21:75,101:125)
ds.tr <- iris[ tr.idx,1:4 ]   # 훈련용
ds.ts <- iris[ -tr.idx,1:4 ]  # 테스트용
cl.tr <- factor(iris[tr.idx,5])
cl.ts <- factor(iris[-tr.idx,5])
pred <- knn(ds.tr,ds.ts,cl.tr,k=3,prob=TRUE)
pred
acc <- mean(pred==cl.ts)
cl.ts <- factor(iris[-tr.idx,5])
pred <- knn(ds.tr,ds.ts,cl.tr,k=3,prob=TRUE)
pred
acc <- mean(pred==cl.ts)
acc
# 훈련용/테스트용 데이터 준비
tr.idx <- c(1:25,21:75,101:125)
ds.tr <- iris[ tr.idx,1:4 ]   # 훈련용
ds.ts <- iris[ -tr.idx,1:4 ]  # 테스트용
cl.ts <- factor(iris[-tr.idx,5])
pred <- knn(ds.tr,ds.ts,cl.tr,k=3,prob=TRUE)
pred
acc <- mean(pred==cl.ts)
cl.ts
cl.tr
cl.ts <- factor(iris[ -tr.idx, 5]) # 테스트 그룹정보
cl.ts
#
# KNN(K-Nearest Neighbor, K-최근접 이웃) 분류 알고리즘
#
library(class)
# 훈련용/테스트용 데이터 준비
tr.idx <- c(1:25,21:75,101:125)
ds.tr <- iris[ tr.idx,1:4 ]   # 훈련용
ds.ts <- iris[ -tr.idx,1:4 ]  # 테스트용
cl.tr <- factor(iris[tr.idx,5])  # 훈련용 그룹정보
cl.ts <- factor(iris[-tr.idx,5]) # 테스트 그룹정보
pred <- knn(ds.tr,ds.ts,cl.tr,k=3,prob=TRUE)
pred
acc <- mean(pred==cl.ts)
acc
table(pred, cl.ts)
