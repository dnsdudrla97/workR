word_data2 <- sapply(word_data, extractNoun, USE.NAMES = F)
word_data2
# 3.1 제대로 추출되지 않은 단어를 사용자 사전에 등록
add_words <- c('백두산','남산','철갑','가을','하늘','달','삼천리','구름') # 달, 삼천리, 구름 안됨 ㅠ
# 사전에서 변환이 안되는 부분은 직접 리스트로 찾아가서 바꾸기
word_data2
buildDictionary(user_dic = data.frame(add_words, rep('ncn',length(add_words))),
replace_usr_dic = T)
get_dictionary('user_dic')
# 3.2 단어 추가 후 다시 명사 추출
word_data2 <- sapply(word_data, extractNoun, USE.NAMES = F)
word_data2
word_data2[[2]][1] <- "동해물"
word_data2[[4]][3] <- "삼천리"
word_data2
#
# R 에서 웹문서 가져오기
#
# 웹에 있는 데이터를 가져오는 단계
#     요청: GET과 POST 방식
#     추출 및 저장
# 관련 R 패키지
#   XML, RCurl, httr, rvest, …
#
#
install.packages( "rvest" )
## 패키지 불러오기
library(rvest)
library(dplyr)
## 변수 입력하기
QUERY <- "제주" # 검색키워드
DATE  <- as.Date( as.character( 20191211 ), format = "%Y%m%d" ) # 검색시작날짜 & 검색종료날짜
DATE  <- format( DATE, "%Y.%m.%d" )
PAGE  <- 1
naver_url_1 <- "https://search.naver.com/search.naver?&where=news&query="
naver_url_2 <- "&pd=3&ds="
naver_url_3 <- "&de="
naver_url_4 <- "&start="
## 날짜 리스트 만들기
DATE_START <- as.Date( as.character( 20191211 ), format = "%Y%m%d" ) # 시작일자
DATE_END   <- as.Date( as.character( 20191211 ), format = "%Y%m%d" ) # 종료일자
DATE <- DATE_START:DATE_END
DATE <- as.Date( DATE, origin = "1970-01-01" )
PAGE <- seq( from = 1, to = 41, by = 10 ) # 시작값과 종료값을 지정해줄 수 있습니다.
PAGE <- seq( from = 1, by = 10, length.out = 5) # 시작값과 원하는 갯수를 지정할 수도 있습니다.
## 네이버 검색결과 url 리스트에서 관련기사 url 리스트 만들기
news_url <- c()
news_date <-c()
for ( date_i in DATE ){
for ( page_i in PAGE ){
dt <- format( as.Date( date_i, origin = "1970-01-01" ), "%Y.%m.%d" )
naver_url <- paste0( naver_url_1, QUERY, naver_url_2, dt, naver_url_3, dt, naver_url_4, page_i )
html <- read_html( naver_url )
temp <- unique( html_nodes( html, '#main_pack' ) %>%     # id= 는 # 을 붙인다
html_nodes( css = '.news ' ) %>%         # class= 는 css= 를 붙인다
html_nodes( css = '.type01' ) %>%
html_nodes( 'a' )%>%
html_attr( 'href' ) )
news_url <- c( news_url, temp )
news_date <- c( news_date, rep( dt, length( temp ) ) )
}
print( dt ) # 진행상황을 알기 위함이니 속도가 느려지면 제외
}
NEWS0 <- as.data.frame( cbind( date = news_date, url = news_url, query = QUERY))
NEWS1 <- NEWS0[ which( grepl( "news.naver.com", NEWS0$url ) ), ]         # 네이버뉴스(news.naver.com)만 대상으로 한다
NEWS1 <- NEWS1[ which( !grepl( "sports.news.naver.com", NEWS1$url ) ), ] # 스포츠뉴스(sports.news.naver.com)는 제외한다
NEWS2 <- NEWS1[ !duplicated( NEWS1 ), ] # 중복된 링크 제거
## 뉴스 페이지에 있는 기사의 제목과 본문을 크롤링
NEWS2$news_title   <- ""
NEWS2$news_content <- ""
for ( i in 1:dim( NEWS2 )[ 1 ] ){
html <- read_html( as.character( NEWS2$url[ i ] ) )
temp_news_title   <- repair_encoding( html_text( html_nodes( html, '#articleTitle' ) ), from = 'utf-8' )
temp_news_content <- repair_encoding( html_text( html_nodes( html, '#articleBodyContents') ), from = 'utf-8' )
if ( length( temp_news_title ) > 0 ) {
NEWS2$news_title[ i ]   <- temp_news_title
NEWS2$news_content[i] <- temp_news_content
}
}
NEWS2$news_content <- gsub( "// flash 오류를 우회하기 위한 함수 추가\nfunction _flash_removeCallback()", "", NEWS2$news_content )
NEWS <- NEWS2 # 최종 결과 저장
NEWS
NEWS$news_content
NEWS$news_content
# 워드클라우드
library( KoNLP )
useSejongDic()
word_data <- sapply( NEWS$news_content, extractNoun, USE.NAMES = F )
word_data
undata <- unlist( word_data )
undata
word_table <- table( undata )
word_table
undata2 <- undata[ nchar( undata ) >= 2 ]
undata2
word_table2 <- table( undata2 )
word_table2
sort( word_table2, decreasing = T )
library( wordcloud2 )
wordcloud2( word_table2, minRotation = -pi / 6, maxRotation = -pi / 6, rotateRatio = 1 )
library(wordcloud)
library(wordcloud2)
library(KoNLP)
library(RColorBrewer)
library(dplyr)
library(ggplot2)
#
# 작성자: 이소정
# 작성일: 2019-12-11
# 제출일: 2019-12-11
#
setwd('D:/workR3')
text1 <- readLines('ex_10-1.txt',encoding = 'UTF-8')
text1
text2 <- readLines('ex_10-2.txt',encoding = 'UTF-8')
text2
text3 <- readLines('ex_10-3.txt',encoding = 'UTF-8')
text3
buildDictionary(ext_dic = 'woorimalsam')
pal2 <- brewer.pal(8,'Dark2')                  # 색상 팔레트 생성
noun1 <- sapply(text1,extractNoun,USE.NAMES = F)
noun1
noun1_1 <- unlist(noun1)
wordcount <- table(noun1_1)
sort.noun <- sort(wordcount,decreasing = T)
sort.noun
sort.noun <- sort.noun[-1]
barplot(sort.noun, names.arg = names(sort.noun),
col='steelblue',main='빈도수 높은 단어',
ylab='단어 빈도수')
wordcloud(names(wordcount),
freq = wordcount,
scale = c(6, 0.7),
min.freq = 3,
random.order = F,
rot.per = .1,
colors = pal2)
noun1_1 <- noun2[nchar(noun1_1)>1]
noun1_1 <- noun1_1[nchar(noun1_1)>1]
noun1_1
noun1_1 <- gsub('비롯', '', noun1_1)
noun1_1 <- gsub('할거', '', noun1_1)
noun1_1 <- gsub('동안', '', noun1_1)
noun1_1 <- gsub('이상', '', noun1_1)
noun1_1 <- gsub('때문', '', noun1_1)
wordcount <- table(noun2)
wordcount <- table(noun1_1)
wordcount
noun1_1 <- gsub('비롯', '', noun1_1)
noun1_1 <- gsub('할거', '', noun1_1)
noun1_1 <- gsub('동안', '', noun1_1)
noun1_1 <- gsub('이상', '', noun1_1)
noun1_1 <- gsub('때문', '', noun1_1)
noun1_1 <- gsub('흔쾌', '', noun1_1)
noun1_1 <- gsub('해서', '', noun1_1)
noun1_1 <- gsub('해도', '', noun1_1)
noun1_1 <- gsub('해매', '', noun1_1)
noun1_1 <- gsub('해내', '', noun1_1)
noun1_1 <- gsub('하면', '', noun1_1)
noun1_1 <- gsub('하고', '', noun1_1)
noun1_1 <- gsub('하길', '', noun1_1)
noun1_1 <- gsub('하신', '', noun1_1)
noun1_1 <- gsub('할만', '', noun1_1)
wordcount <- table(noun1_1)
wordcloud(names(wordcount),
freq = wordcount,
scale = c(6, 0.7),
min.freq = 3,
random.order = F,
rot.per = .1,
colors = pal3)
wordcloud(names(wordcount),
freq = wordcount,
scale = c(6, 0.7),
min.freq = 3,
random.order = F,
rot.per = .1,
colors = pal2)
noun1_1 <- gsub('들이', '', noun1_1)
wordcount <- table(noun1_1)
wordcloud(names(wordcount),
freq = wordcount,
scale = c(6, 0.7),
min.freq = 3,
random.order = F,
rot.per = .1,
colors = pal2)
noun2 <- sapply(text2,extractNoun,USE.NAMES = F)
noun2
noun2_1 <- unlist(noun2)
wordcount2 <- table(noun2_1)
sort.noun2 <- sort(wordcount2,decreasing = T)
sort.noun2
sort.noun2 <- sort.noun2[-1]
sort.noun2
wordcloud(names(wordcount2),
freq = wordcount2,
scale = c(6, 0.7),
min.freq = 3,
random.order = F,
rot.per = .1,
colors = pal2)
noun1_1 <- noun1_1[nchar(noun1_1)>1]
wordcloud(names(wordcount2),
freq = wordcount2,
scale = c(6, 0.7),
min.freq = 3,
random.order = F,
rot.per = .1,
colors = pal2)
noun1_1 <- gsub('한', '', noun1_1)
noun1_1 <- gsub('것', '', noun1_1)
noun1_1 <- gsub('당', '', noun1_1)
wordcount <- table(noun1_1)
wordcloud(names(wordcount2),
freq = wordcount2,
scale = c(6, 0.7),
min.freq = 3,
random.order = F,
rot.per = .1,
colors = pal2)
noun1_1 <- noun1_1[nchar(noun1_1)>1]
noun1_1 <- noun1_1[nchar(noun1_1)>1]
wordcount <- table(noun1_1)
wordcloud(names(wordcount),
freq = wordcount,
scale = c(6, 0.7),
min.freq = 3,
random.order = F,
rot.per = .1,
colors = pal2)
sort.noun2 <- sort(wordcount2,decreasing = T)
sort.noun2
noun1_1 <- noun1_1[nchar(noun1_1)>1]
wordcount <- table(noun1_1)
noun2_1 <- noun2_1[nchar(noun2_1)>1]
wordcount2 <- table(noun2_1)
sort.noun2 <- sort(wordcount2,decreasing = T)
sort.noun2
source('D:/workR3/LSJ_1211.R', encoding = 'UTF-8', echo=TRUE)
wordcloud(names(wordcount2),
freq = wordcount2,
scale = c(6, 0.7),
min.freq = 3,
random.order = F,
rot.per = .1,
colors = pal2)
sort.noun2
noun2_1 <- noun2_1[nchar(noun2_1)>1]
wordcount2 <- table(noun2_1)
wordcloud(names(wordcount2),
freq = wordcount2,
scale = c(6, 0.7),
min.freq = 3,
random.order = F,
rot.per = .1,
colors = pal2)
sort.noun2 <- sort(wordcount2,decreasing = T)
sort.noun2
noun2_1 <- gsub('들이', '', noun2_1)
noun2_1 <- gsub('70', '', noun2_1)
noun2_1 <- gsub('해서', '', noun2_1)
noun2_1 <- gsub('하기', '', noun2_1)
noun2_1 <- gsub('때문', '', noun2_1)
noun2_1 <- gsub('하지', '', noun2_1)
noun2_1 <- gsub('비롯', '', noun2_1)
wordcount2 <- table(noun2_1)
wordcloud(names(wordcount2),
freq = wordcount2,
scale = c(6, 0.7),
min.freq = 3,
random.order = F,
rot.per = .1,
colors = pal2)
wordcloud(names(wordcount2),
freq = wordcount2,
scale = c(6, 0.7),
min.freq = 3,
random.order = F,
rot.per = .1,
colors = pal2)
wordcloud(names(wordcount),
freq = wordcount,
scale = c(6, 0.7),
min.freq = 3,
random.order = F,
rot.per = .1,
colors = pal2)
noun3 <- sapply(text3,extractNoun,USE.NAMES = F)
noun3
noun3_1 <- unlist(noun3)
wordcount3 <- table(noun3_1)
sort.noun3 <- sort(wordcount3,decreasing = T)
sort.noun3
sort.noun3 <- sort.noun3[-1]
wordcloud(names(wordcount3),
freq = wordcount3,
scale = c(6, 0.7),
min.freq = 3,
random.order = F,
rot.per = .1,
colors = pal2)
noun3_1 <- noun3_1[nchar(noun3_1)>1]
wordcount3 <- table(noun3_1)
wordcloud(names(wordcount3),
freq = wordcount3,
scale = c(6, 0.7),
min.freq = 3,
random.order = F,
rot.per = .1,
colors = pal2)
sort.noun3 <- sort(wordcount3,decreasing = T)
sort.noun3
noun3_1 <- gsub('들이', '', noun3_1)
noun3_1 <- gsub('20', '', noun3_1)
noun3_1 <- gsub('때문', '', noun3_1)
noun3_1 <- gsub('하지', '', noun3_1)
noun3_1 <- gsub('동안', '', noun3_1)
noun3_1 <- gsub('들이', '', noun3_1)
noun3_1 <- gsub('하면', '', noun3_1)
noun3_1 <- gsub('하자', '', noun3_1)
noun3_1 <- gsub('해서', '', noun3_1)
wordcount3 <- table(noun3_1)
wordcloud(names(wordcount3),
freq = wordcount3,
scale = c(6, 0.7),
min.freq = 3,
random.order = F,
rot.per = .1,
colors = pal2)
wordcloud(names(wordcount3),
freq = wordcount3,
scale = c(6, 0.7),
min.freq = 3,
random.order = F,
rot.per = .1,
colors = pal2)
text4 <- readLines('ex_10-4.txt',encoding = 'UTF-8')
text4
# KoNLP내에 내장된 사전
useSystemDic()
noun4 <- sapply(text4, extractNoun, USE.NAMES = F)
noun4
word_table <- table(noun4)
undata <- unlist(noun4)
undata
word_table <- table(undata)
word_table
noun4_1 <- noun4_1[nchar(noun4_1)>1]
undata <- unlist(noun4)
undata
word_table <- table(undata)
word_table
undata <- undata[-1]
word_table <- table(undata)
word_table
undata <- undata[-1]
word_table <- table(undata)
word_table
undata4 <- undata[-1]
word_table4 <- table(undata4)
word_table4
buildDictionary(ext_dic = 'woorimalsam')
pal2 <- brewer.pal(8,'Dark2')
noun4 <- sapply(text4,extractNoun,USE.NAMES = F)
noun4
noun4_1 <- unlist(noun4)
wordcount4 <- table(noun4_1)
sort.noun4 <- sort(wordcount4,decreasing = T)
sort.noun4
sort.noun4 <- sort.noun[-3]
sort.noun4
sort.noun4 <- sort.noun4[-3]
sort.noun4
text4 <- readLines('ex_10-4.txt',encoding = 'UTF-8')
text4
noun4 <- sapply(text4,extractNoun,USE.NAMES = F)
noun4
noun4_1 <- unlist(noun4)
wordcount4 <- table(noun4_1)
sort.noun4 <- sort(wordcount4,decreasing = T)
sort.noun4
sort.noun4 <- sort.noun4[-3]
sort.noun4
wordcloud(names(wordcount4),
freq = wordcount4,
scale = c(6, 0.7),
min.freq = 3,
random.order = F,
rot.per = .1,
colors = pal2)
noun4_1 <- noun4_1[nchar(noun4_1)>1]
wordcount <- table(noun4_1)
wordcloud(names(wordcount4),
freq = wordcount4,
scale = c(6, 0.7),
min.freq = 3,
random.order = F,
rot.per = .1,
colors = pal2)
wordcount4 <- table(noun4_1)
wordcloud(names(wordcount4),
freq = wordcount4,
scale = c(6, 0.7),
min.freq = 3,
random.order = F,
rot.per = .1,
colors = pal2)
wordcount4 <- table(noun4_1)
sort.noun4 <- sort(wordcount4,decreasing = T)
sort.noun4
noun4_1 <- gsub('비롯', '', noun4_1)
noun4_1 <- gsub('비롯', '', noun4_1)wordcount4 <- table(noun4_1)
noun4_1 <- gsub('그것', '', noun4_1)
noun4_1 <- gsub('하지', '', noun4_1)
noun4_1 <- gsub('가지', '', noun4_1)
noun4_1 <- gsub('개월', '', noun4_1)
noun4_1 <- gsub('들이', '', noun4_1)
noun4_1 <- gsub('때문', '', noun4_1)
noun4_1 <- gsub('점들', '', noun4_1)
noun4_1 <- gsub('10', '', noun4_1)
noun4_1 <- gsub('번째', '', noun4_1)
noun4_1 <- gsub('그날', '', noun4_1)
noun4_1 <- gsub('년대', '', noun4_1)
noun4_1 <- gsub('동안', '', noun4_1)
noun4_1 <- gsub('이것', '', noun4_1)
noun4_1 <- gsub('17', '', noun4_1)
noun4_1 <- gsub('18', '', noun4_1)
noun4_1 <- gsub('11km나', '', noun4_1)
noun4_1 <- gsub('1960', '', noun4_1)
noun4_1 <- gsub('1970', '', noun4_1)
noun4_1 <- gsub('20', '', noun4_1)
noun4_1 <- gsub('33', '', noun4_1)
noun4_1 <- gsub('35', '', noun4_1)
noun4_1 <- gsub('4000', '', noun4_1)
noun4_1 <- gsub('7시30분에', '', noun4_1)
noun4_1 <- gsub('그걸', '', noun4_1)
noun4_1 <- gsub('그땐', '', noun4_1)
noun4_1 <- gsub('만큼', '', noun4_1)
noun4_1 <- gsub('말하', '', noun4_1)
noun4_1 <- gsub('해서', '', noun4_1)
noun4_1 <- gsub('하시', '', noun4_1)
noun4_1 <- gsub('하라', '', noun4_1)
noun4_1 <- gsub('하길', '', noun4_1)
wordcount4 <- table(noun4_1)
wordcloud(names(wordcount4),
freq = wordcount4,
scale = c(6, 0.7),
min.freq = 3,
random.order = F,
rot.per = .1,
colors = pal2)
wordcloud(names(wordcount4),
freq = wordcount4,
scale = c(6, 0.7),
min.freq = 2,
random.order = F,
rot.per = .1,
colors = pal2)
noun4_1 <- gsub('\\d+', '', noun4_1)
text5 <- readLines('ex_10-5.txt',encoding = 'UTF-8')
text5
noun5 <- sapply(text5,extractNoun,USE.NAMES = F)
noun5
noun5_1 <- unlist(noun5)
wordcount5 <- table(noun5_1)
sort.noun5 <- sort(wordcount5,decreasing = T)
sort.noun5
sort.noun5 <- sort.noun5[-1]
sort.noun5
noun5_1 <- noun5_1[nchar(noun5_1)>1]
wordcount5 <- table(noun5_1)
wordcount5 <- table(noun5_1)
sort.noun5 <- sort(wordcount5,decreasing = T)
sort.noun5
noun5_1 <- gsub('들이', '', noun5_1)
noun5_1 <- gsub('들이', '', noun5_1)wordcount5 <- table(noun5_1)
noun5_1 <- noun5_1[nchar(noun5_1)>1]
noun5_1 <- gsub('들이', '', noun5_1)
noun5_1 <- gsub('때문', '', noun5_1)
noun5_1 <- gsub('동안', '', noun5_1)
noun5_1 <- gsub('이것', '', noun5_1)
noun5_1 <- gsub('말하', '', noun5_1)
noun5_1 <- gsub('//d+', '', noun5_1)
wordcount5 <- table(noun5_1)
wordcloud(names(wordcount5),
freq = wordcount5,
scale = c(6, 0.7),
min.freq = 2,
random.order = F,
rot.per = .1,
colors = pal2)
is.list(noun5_1)
is.vector(noun5_1)
noun5_1 <- gsub('미국은', '미국', noun5_1)
wordcount5 <- table(noun5_1)
sort.noun5 <- sort(wordcount5,decreasing = T)
sort.noun5
wordcloud(names(wordcount5),
freq = wordcount5,
scale = c(6, 0.7),
min.freq = 2,
random.order = F,
rot.per = .1,
colors = pal2)
wordcloud2(names(wordcount5),
freq = wordcount5,
scale = c(6, 0.7),
min.freq = 2,
random.order = F,
rot.per = .1,
colors = pal2)
